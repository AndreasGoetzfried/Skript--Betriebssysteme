\documentclass{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage[ngerman]{babel}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}
\begin{titlepage}
\centering
    \begin{figure}
    \centering
	    \includegraphics[width=90mm]{logo_lmu.jpg}
    \end{figure}
	{\scshape\LARGE Ludwig-Maximilians Universität \par}
	\vspace{1cm}
	{\scshape\Large Skript \par}
	\vspace{1.5cm}
	{\huge\bfseries Betriebssysteme\par}
	\vspace{2cm}
	{\Large\itshape Andreas Götzfried\par}
    \vfill
	    basierend auf\par
	    Prof. Dr. C.\textsc{Linnhoff-Popien}
    \vfill
	{\large \today\par}
\end{titlepage}
\tableofcontents{}

\newpage
\section{Einführung}
\subsection{Das Betriebssystem}
\subsubsection{Einordnung der Maschinensprache}
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/1_Kapitel/Hierarchie-der-Funktionen.png}
    \end{figure}
\subsubsection{Aufgaben des Betriebssystems}
    Ein Betriebssystem soll die Komplexität der Maschinensprache mindern. Das Betriebssystem ist eine \textbf{erweiterte Maschine}, die leichter zu programmieren ist als die darunter liegenden Schichten. Außerdem gilt es als \textbf{Ressourcenmanager}, indem es die vorhandenen Systemressourcen (Speicher, CPU, ...) verwaltet und zwischen den Anforderungen der Programme und diesen Ressourcen vermittelt. Für das \textbf{Multiplexing}, das Verteilen von Ressourcen auf mehrere Programme beziehungsweise Nutzer, kann ein \textbf{Time Multiplexing} gewählt werden, das die Ressourcen zeitlich verzahnt vollständig einem Programm zur Verfügung stellt (Prozessor), oder das \textbf{Space Multiplexing}, bei welchem sich mehrere Programme gleichzeitig die Ressourcen teilen (Hauptspeicher). Auch dient das Betriebssystem als \textbf{Kontrollinstanz} für jegliche Zugriffe und unterbindet unerlaubte Zugriffe.\newline
    \textbf{Bestandteile:}\newline
    Ein Betriebssystem besteht aus verschiedenen Programmen, die im Systemmodus(Kernel Mode) priorisiert laufen. Dazu zählen Gerätetreiber, Prozessmanager, Festermanager und andere.\newline
    \textbf{Systemaufrufe:}\newline
    Die Schnittstelle zwischen Anwendungsprogrammen und Betriebssystem wird durch Systemaufrufe definiert. 
\subsubsection{Geschichte der Betriebssysteme}
    \textbf{1. Generation: 1945 bis 1955 (Röhren und Klinkenfelder}\newline
    John von Neumann und Konrad Zuse waren Größen dieser Zeit, die riesige Apparate mit zehntausenden von Röhren und Taktzeiten in Sekunden als erste Rechner herstellten. Programme wurden über Klinkenfelder, später durch Lochkarten ersetzt, gesteckt oder in Maschinencode eingegeben. Programmiersprachen, Assemblersprachen und Betriebssysteme waren unbekannt.\newline
    \textbf{2. Generation: 1955 bis 1965 (Transistoren und Stapelsysteme}\newline
    Die Zuverlässigkeit von Systemen (Mainframe) wurde durch Transistoren erhöht, waren aber extrem teuer. Große Leerlaufzeiten durch die Programme, die durch Lochkarten eingegeben wurden und bis zum Ende durchlaufen mussten, entstanden. Deshalb wurde das Stapelverarbeitungssystem (Batchsystem) eingeführt, welches Aufträge sammelt und vom Mainframe eingelesen wurde. Ein Programmstapel begann mit einer \$JOB Karte, die die maximale Laufzeit, die Abrechnungsnummer und Name des Programmierer übertrug, dann folgte die \$FORTRAN Karte, die das Betriebssystem beauftragte, den Fortran Compiler zu laden, danach das zu übersetzende Programm und eine \$LOAD Karte, die das Programm lud, die \$RUN Karte veranlasste das Betriebssystem, das Programm zu bearbeiten, und mit der \$END Karte wurde die Programmbearbeitung beendet.\newline
    Erste Betriebssysteme waren FMS (Fortran Monitor System) und das IBSYS (IBM Betriebssystem). Diese Betriebssysteme unterbrachen bei jeder I/O-Aktion die CPU.\newline
    \textbf{3. Generation: 1965 bis 1980 (Integrierte Bauelemente und Multiprogramming}\newline
    Diese Generation verwendete integrierte Bauteile. Die Fortschritte waren das \textbf{Spooling}, das Jobs auf Vorrat einlesen und auf Festplatten speichern konnte, und der \textbf{Mehrprogrammbetrieb}, der Space Multiplexing verwendete.\newline
    Da man keinen Zugriff auf das Programm hatte, war ein Dialogbetrieb nötig, deshalb wurde das \textbf{Timesharing}-System (CTSS) entwickelt, welches jedem Benutzer über ein Terminal auf den Rechner zugreifen ließ. Dann wurde \textbf{MULTICS} (Multiplexed Information und Computing Service), ein Timesharing-System, das nur bei Bedarf die Kapazität nimmt, die es benötigt. Eine Einbenutzerversion \textbf{UNICS} (Uniplexed Information und Computing System) später \textbf{UNIX} (in C programmiert) wurde später vorgestellt.\newline
    \textbf{4. Generation: seit 1980 (PCs)}\newline
    Die Entwicklung von LSI (Large Scale Integration) Schaltkreisen erhöhte die Rechenleistung. Betriebssysteme waren \textbf{MS-DOS} (Microsoft Disk Operation System) und UNIX. zusätzlich wurde die \textbf{GUI} (Graphical User Interface) erfunden.\newline
    Betriebssysteme für Rechner lassen sich nun unterteilen:
    \begin{itemize}
        \item Netzwerk-Betriebssystem: Der Benutzer kennt mehrere Rechner mit eigenem Betriebssystem und kann sich gezielt auf diesen anmelden
        \item Verteiltes Betriebssystem: Soll wie ein Einprozessorsystem erscheinen, damit die Nutzer nicht wissen, wo Programme ablaufen und Daten liegen
    \end{itemize}
    \textbf{5. Generation: seit circa 2000 (mobile Betriebssysteme)}\newline
    Erstes Smartphone ist der Simon Personal Computer von IBM und BellSouth. Ab den iPhones wurde der Touch Screen populär. Anforderungen waren energiesparender und effizienter Zugriff wegen den begrenzten Ressourcen. Android basiert auf stark angepasstem Linux-Kernel mit besonders ressourcenschonender Programmierung. Die Sicherheit wird über eine Distributionskanal des Betriebssystems gewährleistet (PlayStore).
\subsubsection{Arten von Betriebssystemen}
    \begin{itemize}
        \item Mainframe-Betriebssystem: Optimiert auf gleichzeitig ablaufende Prozesse mit vielen I/O-Operationen
        \item Server-Betriebssystem: Kleine Webserver und Workstations
        \item Multiprozessor-Betriebssystem: Bei mehreren Prozessoren wird ein spezielles Betriebssystem benötigt, das die Verteilung der Aufgaben an die Prozessoren übernimmt
        \item PC-Betriebssystem: Generische Plattform für viele Anwendungen und Anforderungen
        \item Echtzeit-Betriebssystem: Steuerung von Maschinen. Weiches oder hartes System, bei geringer oder größerer Toleranz bei I/O-Operationen
        \item Embedded Betriebssystem: Für PDAs und mobile Endgeräte entwickelt; Kommunikationsschnittstellen sind wichtig
        \item Betriebssystem für Chipkarten: Mini-Betriebssystem, z.B. im Zusammenhang mit RFID-Technologie
    \end{itemize}
    
\newpage
\section{Prozesse}
\subsection{Programme und Unterprogramme}
\subsubsection{Vom Programm zum Maschinenprogramm}
    Ein zur Ausführung befindliches Programm ist ein Folge von Befehlen, die vom Prozessor ausgeführt werden. Um eine höhere Programmiersprache in Maschinenprogramm zu übersetzen, wird ein Compiler benötigt. Soll das Maschinenprogramm ausgeführt werden, bekommt er einen Speicherbereich zugewiesen mit genau so viel Speicherzellen wie Befehle im Programm, in welchen dann die Befehle kopiert werden.  
\subsubsection{Unterprogramme und Prozeduren}
    \textbf{Offenes Unterprogramm:}\newline
    Der Programmtext wird in das Hauptprogramm kopiert. Probleme beim Ändern des Unterprogramms und bei langen Unterprogrammen (Speicherplatz).\newline
    \textbf{Geschlossenes Unterprogramm (Prozedur):}\newline
    Das Programm wird über seine Anfangsadresse angesprungen und bei beenden über die Rückkehradresse verlassen. Announcements sind Prozeduren ohne Ergebnisparameter, Invocation mit. Auch rekursive Unterprogramme sind möglich.\newline
    Die Anfangsadresse wird mit dem Befehl CALL direkt angesprungen. RET bewirkt den Rücksprung. Die Prozedur kann Aufrufparameter und Rückgabewerte besitzen.\newline
    Die Informationen werden entweder durch ein Stack ausgetauscht, die eine beliebige Anzahl von Parametern definiert, oder durch spezielle Register, die in der Speicherhierarchie ganz oben stehen und geringste Zugriffszeiten haben.\newline
    \\
    \textbf{2.1.2.1 Die Befehle CALL und RET}\newline
    \begin{lstlisting}
        COMMAND JMP addr
        BEGIN 
            PC := addr;
        END
    \end{lstlisting}
    Der Befehl CALL unterscheidet sich vom Befehl JUMP durch die Sicherung der Rücksprungadresse. Entweder wird diese in einem Register gesichert oder auf einem Stack.
    \begin{lstlisting}
        COMMAND CALL addr
        BEGIN 
            RA := PC + 1 | PUSH (PC + 1);
            PC := addr;
        END
    \end{lstlisting}
    Entsprechend beim RET Befehl.
    \begin{lstlisting}
        COMMAND RET
        BEGIN 
            PC := RA | PC := POP;
        END
    \end{lstlisting}
    \\
    \textbf{2.1.2.2 Schema für Unterprogrammaufrufe}\newline
    Eine \textbf{Nested Procedure} ruft in ihrem eigenen Programm ein weiteres Unterprogramm auf, wobei immer wieder die RET-Adresse auf das aufrufende Programm weitergegeben werden muss.  
    \\
    \textbf{2.1.2.3 Module}\newline
    Module sind zum Beispiel Unterprogramme, Komponenten des Betriebssystems, Benutzerprogramme oder Prozesse. Zu einem Zeitpunkt kann nur ein Modul einem Rechnerkern zugeordnet werden. Die Zustände von Modulen, den Rechnerkernzustand, Speicherabbildungstabellen, Programmcode und Daten, müssen verwaltet werden. Eine Aktivierung eines Moduls kann durch einen Unterprogramm-/Prozeduraufruf, einem Systemaufruf oder einem Prozesswechsel erreicht werden.\newline
    \textbf{Server und Client:}\newline
    In verteilten Systemen stellen Module auch über Rechnergrenzen hinweg Dienste bereit.\newline
    \textbf{Dienst;}\newline
    Ein Dienst ist eine Funktion, die von einem Objekt an einer Schnittstelle angeboten wird.\newline
    \textbf{Entfernte Prozeduraufrufe:}\newline
    Da sich zwei Computer nicht den gleichen Speicher teilen, benötigt man entfernte Prozeduraufrufe, um Module aufrufen zu können.
\subsubsection{Realisierung eines Unterprogrammaufrufs}
    Die Modell-Maschine MI verfügt über die Register PC (Programmzähler), SP (Stack-Pointer) und R0 bis R14, wobei R12/R13 nicht frei genutzt werden dürfen, sondern die Ablageadresse des ersten Aufrufparameters und die Basisadresse des lokalen Datenraums des Unterprogramms erhalten. Außerdem besitzt die MI noch einen Kellerspeicher mit ausreichendem Speicherplatz. Die Adressen der Kellerzellen werden bei wachsendem Keller kleiner. Alle Speicherzellen sind 4 Bytes breit und können damit alle Maschinenbefehle zu 32-Bit-Wörtern kodiert.\newline
    Die Parameter des Unterprogrammaufrufs werden in umgekehrter Reihenfolge auf den Keller gelegt (lokaler Datenraum) und der Rückgabewert bekommt ebenfalls einen Kellerplatz. Die Register der Maschine MI sind Callee-saved, dass heißt das aufgerufene Unterprogramm trägt die Verantwortung dafür, dass der Kontext des Aufrufers nach dem Rücksprung unverändert dem Kontext vor dem Unterprogrammaufruf entspricht.\newline
    Befehle der MI sind:
    \begin{itemize}
        \item PUSH val: Legt Wert auf den Keller
        \item PUSH reg: Legt Inhalt des Registers auf Keller
        \item PUSHR: Sichert gesamen CPU-Register-Kontext (R0 bis R14)
        \item POP reg: Legt Inhalt der obersten Kellerzelle in Register
        \item POPR: Stellt gesamten Register-Kontext her
        \item MOVE addr, reg: Kopiert Adresse ins Register
        \item MOVE reg1, reg2: Kopiert Inhalt von Register 1 ins Register 2
        \item CALL addr: Sprung zu Adresse und Sicherung der Rücksprungadresse auf Keller
        \item RET: Rücksprung zum Aufrufer
    \end{itemize}
    Parameter können vom Hauptprogramm an das Unterprogramm und umgekehrt auf die Art Call by value (Wertübergabe) oder Call by referenz (Adressübergabe).
\subsubsection{Rekursive Prozeduraufrufe}
    Zu beachten ist hierbei, dass im i-ten Unterprogramm das Ergebnis in den lokalen Datenraum des (i-1)-ten Unterprogramms geschrieben wird. Daher ist es so wichtig, erst Speicherplatz für ein Ergebnis einzurichten und dann zu springen.
\subsection{Prozesse}
\subsubsection{Das Prozess-Konzept}
    \textbf{2.2.1.1 Grundlagen von Prozessen}\newline
    Im Vergleich zu Berechnungen durch den Prozess sind Operationen auf E/A-Geräten oft erheblich langsamer. Es soll verhindert werden, dass die CPU beim Warten auf das Ende von E/A-Operationen still steht.\newline
    Ein \textbf{Prozess} ist ein in Ausführung befindliches Maschinenprogramm mit aktuellem Wert des Programmzählers und den aktuellen Werten der Register und der Variablen.\newline
    Zum \textbf{Prozesskontext} gehören alle Informationen, die den aktuellen Ausführungszustand eines Prozessen genau beschreiben. Dazu zählt die CPU-Register-Belegungen und die Prozess-Status-Informationen. Hiermit lässt sich ein unterbrochener Prozess wieder fortsetzen.\newline
    Als \textbf{Image} eines Prozesses bezeichnet man die Gesamtheit der physischen Bestandteile eines Prozesses, also die Befehlsfolge und Kontext mit lokalen und globalen Variablen und Ausführungsstack. Hiermit lässt sich ein Prozess auf einem anderen Computer fortsetzen.\newline
    Beim \textbf{Uniprogramming} werden Prozesse sequenziell nacheinander, vollständig und ohne Unterbrechung ausgeführt.\newline
    Beim \textbf{Multiprogramming} wird zwischen mehreren Prozessen pseudo-parallel hin- und hergeschalten.\newline
    Beim \textbf{Multiprocessoring} stehen mehrere Prozessoren zur Verfügung, damit Prozesse echt-parallel ausgeführt werden können.\newline
    \\
    \textbf{2.2.1.2 Erzeugung von Prozessen}\newline
    Mit \textit{fork} wird eine identische Kopie (Kindprozess) des Prozesses unter Unix erzeugt. Unter MS DOS können Vater- und Kindprozesse nicht parallel ausgeführt werden. Hier wird der Vaterprozess suspendiert bis der Kindprozess fertig ist.\newline
    Durch \textbf{Dispatching} wird ein Rechnerkern an einen Prozess zugeordnet.\newline
    Die Ursachen für die Erzeugung eines Prozesses sind neue Stapelaufträge, die Benutzeranmeldung, ein Dienstleistungsprozess oder Kindprozesse.\newline
    In Unix sind alle Prozesse Nachkommen des \textit{init}-Prozesses.\newline
    Um einen Prozess zu erzeugen muss zuerst der Prozess einen Identifikator zugewiesen bekommen und mit dieser PID in die Prozesstabelle eingetragen. Dem Prozess wird dann Speicherplatz für das Prozess-Image zugeordnet. Danach wird der Prozesskontrollblock (PCB) initialisiert, die erforderlichen Links gesetzt und in eine Liste eingefügt werden (Ready bz. Ready, Suspend). Gegebenenfalls sind Datenstrukturen zu erweitern.\newline
    \\
    \textbf{2.2.1.3 Realisierung von Multiprogramming}\newline
    Aus Sicht des Prozessors ist es egal, welchen Code er ausführt und für ein Programm ist es nur wichtig, dass die Reihenfolge der Befehle eingehalten wird. Der \textbf{Dispatcher} ist ein Prozess, der einen Prozess unterbrechen und einem anderen Prozess dem Prozessor zuordnen kann.\newline
    \\
    \textbf{2.2.1.4 Das 2-Zustands-Prozessmodell}\newline
    Mit \textbf{Running} und \textbf{Not running} wird dieses einfache Prozessmodell beschrieben.
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/2-Zustandsmodell.png}
    \end{figure}
    Zwei Anforderungen an Information müssen abgeleitet werden können. Der aktuelle Zustand des Prozess muss beschreibbar sein und die Speicherinformationen des Prozesses müssen abrufbar sein.\newline
    Der \textbf{Scheduler} setzt die Strategie um, nach der entschieden wird, wann welcher Prozess als nächstes rechnen darf. Der \textbf{Dispatcher} übernimmt das Suspendieren und die Zuweisung der CPU an einen Prozess.\newline
    \\
    \textbf{2.2.1.5 Das 5-Zustands-Prozessmodell}\newline
    Hier kommen Ready, Blocked, New und Exit dazu und Not running ist überflüssig.
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/5-Zustandsmodell.png}
    \end{figure}
    Die Blocked Queue kann auch noch in priorisierte Queue eingeteilt werden.\newline
    \\
    \textbf{2.2.1.6 Das 7-Zustands-Prozessmodell}\newline
    Wenn der Hauptspeicher alle Prozesse in die Blocked Queue schiebt, steht kein Speicherplatz für weitere Prozesse zur Verfügung. Dieses Problem kann durch größeren Hauptspeicher oder Swapping gelöst werden.\newline
    \textbf{Swapping} lagert Teile eines blockierten Prozesses auf die Platte aus.\newline
    Der \textbf{Virtuelle Speicher} ist die Menge an Speicher, die dem Betriebssystem maximal zur Abbildung von Prozessen auf dem Hintergrundspeicher zur Verfügung stehen.\newline
    Das Swapping kann auch schon im 5-Zustands-Modell eingeführt werden.
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/5-Zustandsmodell-mit-Suspend.png}
    \end{figure}
    Wenn man bei diesem Modell sowohl bei den im Hauptspeicher enthaltenen als auch bei den ausgelagerten Prozessen zwischen blockierten und nicht blockierten unterscheidet, entsteht das 7-Zustands-Modell.
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/7-Zustandsmodell.png}
    \end{figure}
\subsubsection{Prozessbeschreibung}
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/Systemressourcen.png}
    \end{figure}
    \textbf{2.2.2.1 Kontrollstrukturen des Betriebssystems}\newline
    Die \textbf{Seitenrahmen (Frames)} beschreiben die Einteilung des Hauptspeichers in Speicherzellblöcken. Auch wird die Instruktionsfolge eines Prozesses in \textbf{Seiten} aufgeteilt.\newline
    Die \textbf{Speichertabellen} dienen dazu, den Überblick über den Haupt- und virtuellen Speicher zu behalten, wobei gleich ein Teil des Hauptspeichers für das Betriebssystem reserviert ist. Sie sind aufgebaut mit der Zuordnung der Seiten eines Prozesses zu den Frames, aus der Zuteilung des virtuellen Speichers zu den Prozessen und den Schutzattributen von Seiten und Frames.\newline
    Die \textbf{E/A-Tabellen} dienen zur Verwaltung der E/A-Geräte. Sie sind entweder vverfügbar oder einem Prozess zugeordnet, wobei hier das Betriebssystem den Status des Geräts kennt.\newline
    Die \textbf{Dateitabellen} enthalten Informationen über die Existenz von Dateien, über ihren Ort im Hintergrundspeicher, Status und Attributen. Falls seperates Dateisystem existiert, so kann ein Großteil dieser Informationen darin enthalten sein.\newline
    Die \textbf{Prozesstabellen} enthalten Informationen zur Verwaltung aller Prozesse.\newline
    \\
    \textbf{2.2.2.2 Prozesskontrollstrukturen}\newline
    Das Betriebssystem muss wissen, wo der Prozess gespeichert ist und welche Werte die relevanten Attribute besitzen.\newline
    Der Prozess kann lokalisiert werden, indem man die \textbf{dynamische Partitionierung (Segmentierung)} einsetzt, welche den Prozess als zusammenhängenden Block in den Speicherzellen ablegt, einen Teil dieser Daten in den Hauptspeicher geladen wird und bei Ausführung alles in den gesamten Prozess in den Hauptspeicher legt. Anders kann es bei einer \textbf{festen Partitionierung (Paging)} ablaufen. Hier wird der Speicher in feste Blöcke partitioniert und ein Prozess-Image in aneinander grenzenden Blöcken gespeichert. Der gesamte Prozess-Image befindet sich hierbei im Hintergrundspeicher und falls ein Teil benötigt wird, wird dieser in freie Frames des Hauptspeichers kopiert.\newline 
    Um die \textbf{Prozessattribute} zu kontollieren wird dies im Prozesskontrollblock (PCB) mit Prozessidentifikation, Prozesszustandsinformation und Prozesskontrollinformation gespeichert. Die \textbf{Prozessidentifikation} enthält den numerischen Identifikator (PID), den Identifikator des Elternprozesses und die ID des Eigentümers. Die \textbf{Prozesszustandsinformationen} beinhalten den Inhalt der Prozessregister und das Programmstatuswort, welches die Menge der Register mit Statusinformationen beschreibt. Die \textbf{Prozesskontrollinformation} besteht aus Scheduling- und Zustandsinformationen wie Prozesszustand, Priorität, Schedling-Strategie und Ereignisse, den Datenstrukturen (z.B. Referenz auf nächsten Prozess), Signalen oder Nachrichten zwischen Prozessen und zusätzlichen Informationen über Privilegien ddes Prozesses, Speichermanagement,  Eigentümerverhältnissen und ähnlichem.\newline
    Die Prozessstruktur im Hintergrundspeicher sieht wie folgt aus.\newline
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/Prozessstruktur_im_Hintergrundspeicher.png}
    \end{figure}
    \\
    \textbf{2.2.2.3 Zusammenfassung der Verwaltung und Beschreibung von Prozessen}\newline
    Das Betriebssystem entscheidet periodisch, ob ein Prozess angehalten werden soll und ein anderer aktiviert werden soll (Scheduler). Um die Arbeit eines suspendierten Prozesses wieder aufnehmen zu können, muss man einen Prozess beschreiben können.
\subsubsection{Prozesskontrolle}
    Es gibt den \textbf{Systemmodus (Kernel Mode)}, bei dem der Prozessor dem Betriebssystem zugeordnet ist, oder den \textbf{Benutzermodus (User Mode)}, bei dem der Prozesor einem Anwendungsprogramm zugeordnert ist. Der Systemmodus ist dabei privilegierter. Das \textbf{Programmstatuswort (PSW)} beschreibt, welcher Modus läuft (0 für Nutzermodus, 1 für Systemmodus).\newline
    Der \textbf{Betriebssystem-Kern} hat folgende Funktionen:
    \begin{itemize}
        \item Support Funktionen: Dienste, die von Nutzerprozessen in Anspruch genommen werden können
        \item Selbstverwaltungs Funktionen: Fehlermanagment, Konfigurationsmanagement, Abrechnungsmanagement, Leistungsmanagment, Sicherheitsmanagement
        \item E/A-Management: Verwaltung von Kanälen und Puffern
        \item Speichermanagement: Segemeniterung, Paging
        \item Prozessmanagement: Prozesserzeugung und Prozesswechsel
    \end{itemize}
    \textbf{2.2.3.1 Prozesswechsel (Kontext-Switch)}\newline
    Die Ursachen, um einen laufenden Prozess zu unterbrechen und einen neuen Prozess einzusetzen, können externe Ereignisse (I/O, timeout, ...), Traps (Ausnahmebedingung, Fehler, ...) oder ein Supervisor-Call (Betriebssystem Funktion) sein.\newline
    Um den Prozess erfolgreich zu wechseln muss zuerst der PCB den bisherigen Prozess aktualisiert und gesichert werden, dann muss der PCB den bisherigen Prozess in die Queue des Schedulers eingefügt werden. Anschließend wird ein anderer Prozess ausgewählt und ausgeführt. Zum Schluss wird der neue Prozess beim PCB wiederhergestellt und aktualisiert (Ausführungszustand auf "running").\newline
    \\
    \textbf{2.2.3.2 Unterbrechungen}\newline
    Eine Unterbrechung ist das Maschinenkonzept für die Behandlung nicht deterministischer Abläufe.\newline
    Man unterscheidet zwischen \textbf{externen Unterbrechungen (Interrupt)} bei Fehlern die außerhalb des Prozesses ausgelöst werden und \textbf{internen Unterbrechungen (Exception)}, welche vom ausgeführten Prozess selbst ausgelöst wird. Die \textbf{Unterbrechungsroutine (Interrupt Handler)} ist eine Komponente des Betriebssystems, die entscheidet, ob bei einer Unterbrechung der Prozess fortgesetzt werden kann oder in einen anderen Prozess gewechselt werden muss.\newline
    \\
    \textbf{2.2.3.3 Moduswechsel}\newline
    Die Schritte beim Moduswechsel beginnen mit der Aktualisierung und Sicherung der Zustandsinformationen des Prozesskontrollblocks, dann wird der eigentliche Moduswechsel durch Freigabe aller Privilegien durchgeführt und abschließend in die Unterbrechungsroutine gesprungen. Ein Moduswechsel ist schneller als ein Prozesswechsel, da die Zustandsinformationen nicht gesichert werden müssen.\newline
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/Prozess_Moduswechsel.png}
    \end{figure}
    \\
    \textbf{2.2.3.4 Konflikte bei Unterbrechungen}\newline
    Ursachen für Konflikte können sein, wenn während der Unterbrechungsbehandlung weitere Unterbrechungen auftreten, oder, wenn gleichzeitig mehrere Unterbrechungswünsche eintreffen.\newline
    Diese können gelöst werden, indem Prioritäten von Unterbrechungen eingeführt werden. Hierbei haben Benutzerprozesse die niedrigste Priorität, bei internen Unterbrechungen wird die Priorität des Prozesses übertragen, bie externen Unterbrechungen werden von 0 bis 31 Prioritäten festgelegt und falls die Priorität einer Unterbrechung höher ist als die Unterbrechungsbehandlung, die gerade läuft, wird diese zurückgestellt.\newline
    \\
    \textbf{2.2.3.5 Ausführung des Betriebssystems}\newline
    Ein Betriebssystem funktioniert wie eine gewöhnliche Computersoftware, das von einem Prozessor ausgeführt wird und muss oft die Kontrolle abgeben und bekommt dann vom Prozessor die Kontrolle wieder zurück. \newline
    Ein Konzept ist das des \textbf{seperaten Kern (Non-Process Kernel)}, welches in älternen Betriebssysteme vorkommt und der Kern des Betriebssystems außerhalb jeden PProzesses aufgeführt wird. Wir ein in Ausführung befindlicher Prozess unterbrochen, muss der Kontext gespeichert werden und die Kontrolle an den Betriebssystem-Kern übergeben. Das Betriebssystem verfügt über einen eigenen Speicher und eigenen Systemkeller zur Kontrolle von Prozeduraufrufe und entsprechenden Rücksprüngen. Ein anderes Konzpet ist das der \textbf{Integration in die Nutzerprozesse (Execution within User Process)}, welcher bei kleineren  Computern gebräuchlich ist und das gesamte Betriebssystem im Kontext jedes Nutzerprozesses ausgeführt wird. Ein Weiteres ist das es \textbf{prozessbasierten Betreibssystem (Process-based Operation System)}, bei welchem das Betriebssystem als eine Sammlung von Systemprozessen implementiert wird.   
\subsection{Threads}
    EinAls Zusammenfassung des Prozesses kann der Prozess als Eigentümer und Verwalter von Ressourcen oder als eine Einheit, die eine gewisse Aufgabe erledigt betrachtet werden. Die zweite Definition wird im folgeden als Thread bezeichnet.
\subsubsection{Multithreading}
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/multithread.png}
    \end{figure}
    Mit einem Prozess wird in der Multithread-Umgebung ein virtueller Adressraum, in dem das Prozessimage abgelegt wird, ein exclusiver Zuganz zum Prozessor, zu Prozessen, zu Dateien, zu E/A-Geräten und Zugriffsrechte assoziiert.\newline
    Innerhalb dieses Prozesses werden mehrere Threads ausgeführt, die dessen Adressraum nutzen. Ein Thread wird durch einen Zustand der Threadausführung, den Thread Kontext, den zwei Stacks für die Ausführung (user/kernel stack), etwas Speicherplatz und dem Zugriff auf Speicher und Ressourcen des Prozesses definiert.\newline
    Im \textbf{Singlethreading} werden die Prozessorregister vom Prozess kontrolliert. Zusätzlich wird beim \textbf{Multithreading} ein Kontrollblock (TCB) für jeden Thread zugeordnet.\newline
    Vorteile:
    \begin{itemize}
        \item weniger Zeit zur Generierung neuer Threads
        \item größere Kommunikation zwischen Threads (Sicherheitsproblem)
        \item weniger Wartezeit bei Blockierungen von einzelnen Threads
        \item weniger Zeit beim Kontextwechsel unter Threads
    \end{itemize}
    Wird ein Prozess geswappt, werden auch alle Threads mit ausgelagert. 
\subsubsection{Threadzustände}
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/threadzustaende.png}
    \end{figure}
\subsubsection{User-Level-Threads (ULT)}
    Bei diesen Threads ist das Threadmanagement Aufgabe der Anwendung und das Betriebssystem kennt diese Threads nicht. Threadbibliotheken ermöglichen die Programmierung von Multithread-Anwendungen. Diese erhalten Code für die Generierung und Terminierung, zur Nachrichten- und Datenübermittlung, zum Scheduling und zum Sichern und Löschen von Threadkontexten.\newline
    Läuft ein Thread in einem Prozess kann in diesem jederzeit ein neuer Prozess generiert werden.\newline
    Vorteile:
    \begin{itemize}
        \item Keine Privilegien für die Threaderzeugung/Threadwechsel
        \item Scheduling kann auf Threadebene realisiert werden
        \item Jedes Betriebssystem kann dieses machen
    \end{itemize}
    Nachteile:
    \begin{itemize}
        \item wird ein Thread blockiert, wird der gesamte Prozess blockiert (Lösung: Jacketings)
        \item keine parallele Ausführung von Threads (Lösung: seperate Prozesse)
    \end{itemize}
\subsubsection{Kernel-Level-Threads (KLT)}
    Dieses Thread Konzept wird vom Betriebssystem-Kern ausgeführt. Vorteile sind die schnelle Erzeugung, Terminiertung und Wechsel von KLTs, verschiedene Prozessoren können für unterschiedliche Threads desselben Prozesses benutzt werden und bei Blockade kann die Kontrolle einem anderen Thread desselben Prozesses übergeben werden. Ein Nachteil ist bei der Kontrollübergabe an einen anderen Thread, dass ein Moduswechsel erforderlich ist und es deshalb zu verlangsamung kommen kann.
\subsubsection{Kombinierte Konzepte}
     Um beide Threadkonzepte zu verwenden, wurden kombinierte Betriebssysteme entwickelt. Ziel ist es dabei, die Vorteile beider Konzepte zu kombinieren und die Nachteile zu minimieren. Im folgenden werden die Verhältnis Threads:Prozesse näher dargestellt.
     \begin{itemize}
         \item 1:1 jedem Thread ist ein Prozess zugeordnet (ältere UNIX-Versionen)
         \item n:1 einem Prozess sind mehrere Threads zugeordnet (Windows NT, Solaris)
         \item 1:n ein Thread kann von einer Prozessumgebung zu anderen migriert werden (Clouds Operating System)
         \item n:m kombiniert n:1 und 1:n Ansätze (Operating System TRIX)
     \end{itemize} 
\subsubsection{Andere Formen paralleler Abläufe}
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/paralleke_prozessoren.png}
    \end{figure}
    Um Parallelität zu ermöglichen, gibt es die Ansätze Master/Slave Architektur, das Symmetrische Multiprocesseing (SMP) und das Clustering.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/parallel_prozessoren_bild.png}
    \end{figure}
    Der SMP-Ansatz hat die Vorteile Verteilter Systeme, aber es muss gewährleistet werden, dass zwei Prozessoren nie denselbsen Prozess auswählen. Beim SMP hat jeder Prozessor seine eigene Kontrolleinheit, ALU und Register. Jeder Prozess hat Zugriff zum gemeinsamen Hauptspeicher und den E/A-Geräten.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/smp.png}
    \end{figure}
\subsection{Scheduling}
\subsubsection{Das Prinzip des Schedulings}
    Scheduling wird notwendig, sobald mindestens ein Prozess im Zustand \textit{running} und mehrere Prozesse im Zustand \textit{ready} sind.
    \textbf{2.4.1.1 Varianten des Schedulings}\newline
    Beim \textbf{nicht preemptiven Scheduling} handelt es sich um eine Art, die beim Stapelbetrieb eingesetzt wird. Deshalb wurde es meist in älternen Systemen benutzt. Das \textbf{preemptive Scheduling} unterbricht jederzeit Prozesse, so dass ein anderer Prozess zur Ausführung kommen kann.\newline
    \\
    \textbf{2.4.1.2 Anforderung an einen Scheduling-Algorithmus}\newline
    Es werden drei Klassen von Systeme unterschieden. Das Batch-System (Stapelverarbeitungssystem), das interaktive System und das Echtzeitsyste.\newline
    Anforderungen, die allen Systemen gemein sind, sind:
    \begin{itemize}
        \item Fairness: gerechter Anteil an der Prozessorzeit
        \item Policy Enforcement: keine Ausnahmen
        \item Balance: alle Teile des Systems sind ausgelastet
        \item Datensicherheit: kein Verlust von Daten
        \item Skalierbarkeit: mittlere Leistung wird beibehalten
        \item Effizienz: Prozessor vollständig ausgelastet
    \end{itemize}
    Beim Batch-System gilt zusätzlich noch der Durchsatz (Maximierung der Prozesse pro Zeiteinheit), die Verweildauer (Minimierung der Zeit vom Start bis Beendigung) und die Prozessorauslastung (maximale Auslastung der CPU). Das Interaktive System benötigt zusätzlich noch minimale Antwortzeit, Verhältnismäßigkeit (Erwartungen des Benutzers müssen berücksichtigt werden) und maximale Anzahl interagierenden Nutzer (Interaktion). Beim Echtzeitsystem sind Sollzeitpunkte (Timeouts) und Vorhersagbarkeit weitere Anforderungen.\newline
    Meist werden die Ready-Queues in Prioritätenlisten aufgesplittet. Damit Prozesse mit niedrigen Priotitäten nicht verhungern, kann sich die Priotitäten durch die Zeit in der Queue ändern.\newline
    Beim \textbf{nonpreemptiven Modus} wird der Prozess bis zur Terminierung oder durch Warten fortgesetzt. Anders beim \textbf{preemptiven Modus}, bei welchen Prozesse aufgrund eines neuen Prozesses, ein höher priorisierten Prozess oder Timeouts terminiert werden.\newline
    \\
    \textbf{2.4.1.3 Scheduling vs. Dispatching}\newline
    Ein Scheduler nimmt die Auswahl eines rechenbereiten Prozesses und die Unterbrechung des aktuell abgearbeiteten Prozesses. Der Dispatcher setzt diesen Kontextwechsel um.
\subsubsection{Scheduling-Algorithmen}
    \textbf{2.4.2.1 Begriffe}\newline
    \begin{itemize}
        \item Ankunftszeit: Zeitpunkt ab Existenz eines Prozesses
        \item Startzeit: Zeitpunkt der ersten Zuweisung an den Prozessor
        \item Verweildauer: Zeitdifferenz zwischen Ankunftszeit und Terminierung
        \item Antwortzeit: Zeitspanne zwischen Eingabe und Beginn des Empfange seiner Antwort
        \item Bedienzeit: Zeit im Zustand \textit{running} 
        \item Wartezeit: Zeit im Zustand \textit{not running}
        \item Beendigungszeit: Zeitpunkt der Terminierung
        \item Normalisierte Verweildauer: Quotient Verweiildauer zu Bedienzeit (sollte bei 1 liegen)
    \end{itemize}\newline
    \\
    \textbf{2.4.2.2 Nicht-preemptive Scheduling-Algorithmus}\newline
    Es gibt das Prinzip \textbf{First Come First Serve (FCFS)}, welches aber schnell blockiert werden kann. Vorteile sind die einfache Implementierung und die Fairness.\newline
    Ein anderes Prinzip ist das \textbf{Shortest Process Next (SPN)/Shortest Job First (SJF)}, was zum Problem des Verhungerns längerer Prozesse führen kann und man muss den Mittelwert ähnlicher abgearbeiteten Aufträge berechnen, um die erwartete Abarbeitungsdauer zu kennen. Ein Vorteil ist die günstige Verweildauer.\newline
    \\
    \textbf{2.4.2.3 Preemptive Scheduling-Algorithmus}\newline
    Um wichtigere Jobs einzuschieben und die Verweildauer und Antwortzeit zu reduzieren ist diese Art der Algorithmen optimal. Diese ermöglichen auch mehr Flexibilität können aber auch zu Inkonsistenzen führen, wenn Jobs für andere Jobs benötigt werden.\newline
    Das Pendant zum SJF ist beim preemptiven Scheduling das Prinzip \textbf{Shortest Remaining Processing Time (SRPT)}. Ein anderes Prinzip ist \textbf{Round Robin}, wobei mit Timeouts gearbeitet wird. Die mittlere Verweildauer ist besser als FCFS, aber schlechter als SJF und SRPT. Dennoch ist RR fair (kein Verhungern) und praktisch implemetierbar, dazu muss aber der Timeout optimal gewählt werden. Bei RR wird ein kürzerer Prozess schneller abgearbeitet.\newline
    \\
    \textbf{2.4.2.4 Priority Scheduling (PS)}\newline
    Beim Priority Scheduling wird der höherpriorisierte Prozess zuerst ausgewählt mit den Vorteilen, dass wichtige Prozesse bevorzugt werden und der praktischen Implementierung. Nachteile sind das Verhungern von niedrig priorisierten Prozessen und die relativ schlechte mittlere Verweildauer.\newline
    \\
    \textbf{2.4.2.5 Multilevel Feedback Queueing}\newline
    Bei diesem Algorithmus wird sowohl mit Prioritätsklassen als auch mit Zeitscheiben gearbeitet. Bei der Inanspruchnahme der gesamten zugewiesenen Zeit wird der Prozess unterbrochen und an das Ende der nächst niedrigeren Queue gesetzt. Wenn der Prozess den Prozessort freiwillig verlässt, landet dieser wieder in der selben Queue. Die Queue werden im RR-Verfahren abgearbeitet.  
\subsubsection{Prozesswechsel}
    Der Dispatcher realisiert diesen Kontextwechsel des Prozessors.
\subsubsection{Arten des Schedulings}
    Es werden \textbf{Short/Medium/Long Term Scheduling} unterschieden.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/2_Kapitel/arten_von_scheduling.png}
    \end{figure}

\newpage
\section{Multiprocessing}
\subsection{Deadlocks bei Prozessen}
\subsubsection{Motivation der Deadlocks anhand zweier Beispiele}
    Wenn vier Autos in eine Kreuzung einfahren und jeweils die anderen Autos am Weiterfahren hindern, sind wir in einer Deadlocksituation.\newline
    Ein System befindet sich im Zustand einer \textbf{Verklemmung (Deadlock)}, wenn sich mindestens zwei Prozesse in einem wechselseitigen Wartezustand befinden und durch die Freigabe aller Betriebsmittel möglicher anderer Prozesse dieser Zustand nicht aufgehoben werden kann. 
\subsubsection{Das Prinzip der Deadlocks}
    Ein \textbf{Deadlock} ist die dauerhafte Blockierung einer Menge von Prozessen, die gemeinsame Systemressourcen nutzen oder miteinander kommunizieren.\newline
    Wir unterscheiden zwischen \textbf{wiederverwendbaren Ressourcen}, welche von genau einem Prozess genutzt werden können und durch diese Nutzung nicht zerstört werden, und den \textbf{verbrauchenden Ressourcen}, welche erzeugt und zerstört werden und deshalb kein Deadlock möglich ist.
\subsubsection{Deadlock Prevention}
    Um Deadlocks zu verhindern können indirekte und direkte Methoden eingesetzt werden.\newline
    Bei der \textbf{indirekten Methode} müssen drei Bedingungen erfüllt werden, damit ein Deadlock eintreten kann. Diese sind Mutual Exclusion (zwei Ressourcen, die nur von einem Prozess gleichzeitig genutzt werden kann; zwei Prozesse und 2 Betriebsmittel), Hold and Wait (ein Prozess behält eine Ressource, während er auf eine weitere Ressource wartet) und No Preemption (eine Ressource kann einem Prozess nicht entzogen werden).\newline
    Bei der \textbf{direkten Methode} wird die genaue Situation betrachtet, die zu einem Deadlock führen kann, dies wird als Circular Wait (es existiert eine geschlossene Kette von Prozessen, so dass jeder Prozess mindestens eine Ressource hält, die von einem anderen Prozess der Kette benötigt wird).\newline
    \textbf{3.1.3.1 Deadlock Avoidance}\newline
    Bei Deadlockvermeidung lässt man die drei Bedingungen außer Acht. Insbesondere werden Unterbrechungen und Nebenläufigkeiten zugelassen. Es wird dynamisch entschieden, ob Ressourcen genutzt werden können. Verfahren sind, dass ein Prozess nicht gestartet werden darf, falls seine Anforderungen zu einem Deadlock führen könnten, oder, dass eine Ressourcenanfrage nicht befürwortet werden darf, falls sie zu einem Deadlock führen könnte.\newline
    Die erste Methode wird \textbf{Prozess Initiation Denial} genannt. Hierzu werden einige Beschreibungen eingeführt. $\text{Ressource}$ beschreibt einen Vektor aller möglicher Ressourcen, die $\text{Verfügbarkeit}$ einen Vektor der Verfügbaren Ressourcen. Ein Prozess belegt mehrere Ressourcen, welcher zu dem Vektor $\text{Claim}_{\text{Prozess}_{P_1}}$, wenn wir mehrere Prozesse nehmen, wird eine Matrix erzeugt. Die Matrix $\text{Allocation}$ beschriebt die Belegung von Ressourcen zu einem bestimmten Zeitpunkt.\newline
    Damit gilt $R_i= V_i + \sum_{k=1}^n A_{ki}$, $C_{ki}\leq R_i$ und $A_{ki}\leq C_{ki}$.
    Somit wird ein neuer Prozess nur gestartet, falls $R_i\geq C{(n+1)_i}+\sum_{k=1}^n C_{ki}$, in diesem Fall ist es möglich alle gerade laufenden Prozesse quasi gleichzeitig maximale Anforderungen an die Ressource stellen würden.\newline
    Die zweite Methode {Ressource Allocation Denial} beschreibt einen Algorithmus eines Systems mit fester Anzahl von Prozessen und Ressourcen. Zu jedem Zeitpunkt sind einem Prozess null oder mehr Ressourcen zugeordnet. Der Zustand des Systems ist die aktuelle Zuordnung von Ressoucren zu einem Prozess und wird durch Vektoren und Matrizen beschrieben. Ein \textbf{sicherer Zustand} herrscht, wenn mindestens eine Folge von Prozessabläufen, die nicht zu einem Deadlock führt, bis zum Ende abgearbeitet werden können.\newline  
    \\
    \textbf{3.1.3.2 Petri-Netze zur Prozessmodellierung}\newline
    Ein \textbf{Netz} ist ein endlicher Graph $X=(S\cup T, F)$ mit $S, T$ disjunkt, $S$ endlicher Menge von Stellen, $T$ endliche Menge von Transitionen und $F\subset (S\times T)\cup (T\times S)$ Menge von Kanten.
    \begin{figure}[h]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/Beispiel_Petrinetz.png}
    \end{figure}\newline
    Ein \textbf{Stelle/Transitions-System} ist ein 6-Tupel $Y=(S,T,F,W,M_0)$ bestehend aus dem Netzt $(S,T,F)$ und den Abbildungen die Kapazität $K:S\rightarrow \mahtbb{N} \cup {\infty}$, das Kantengewicht $W:F\rightarrow \mathbb{N}$ und die Anzahl der Marken $M:S\rightarrow \mathbb{N}$, zusätzlich gilt $\forall_{s\in S} M(s)\leq K(s)$.\newline
    Die \textbf{Stellen} eines Petrinetztes repräsentieren Signale, Daten, ..., wohingegen die \textbf{Transitionen} Ereignisse, Vorgänge, ... beschreiben.\newline
    Definitionen:
    \begin{itemize}
        \item Vorbereich: $x:= {y\in S\cup T:(y,x)\in F}$
        \item Nachbereich: $x:= {y\in S\cup T:(x,y)\in F}$
        \item Menge aller Eingangskanten: $-x:= {(y,x)\in (S\times T)\cup (T\times S):(y,x)\in F}$
        \item Menge aller Ausgangskanten: $x-:= {(x,y)\in (S\times T)\cup (T\times S):(x,y)\in F}$
    \end{itemize}\newline
    \\
    \textbf{3.1.3.3 Markierungen}\newline
    Markierung können einer Menge aktiverter Transitionen zugeordnet werden, $T_{akt}(M)={t\in T:(\forall_{s\in t}:M(s)\geq W(s,t))\wedge (\forall_{s\in t}M(s)+W(t,s)\leq K(s))}$.\newline
    \\
    \textbf{3.1.3.4 Modellierung von nebenläufigen Prozessen}\newline
    Ein Erzeuger/Verbraucher-Problem definiert sich durch die Probleme, dass der Verbraucher nur auf produzierte Daten vom Speicher zugreifen kann, der Erzeuger nur bei freien Plätzen ablegen darf und der Speicher nicht gleichzeitig von beiden verändert werden darf. Zur Realisierung dieses Problems werden drei Semaphoren benötigt (Modellierung des freien Platzes, des Speicherzugriffs und des belegten Platzes).\newline
    \\
    \textbf{3.1.3.5 Deadlock Detection (Deadlockerkennung)}\newline
    Dies prüft, ob ein Circular Wait vorliegt. Sei dies der Fall, werden Recovering-Strategien angewandt. Um \textit{teilweise} und \textit{vollständige Verklemmungen} durch Analyse von Petri-Netzen erkennen zu können, wird ein \textbf{Ereichbarkeitsgraph} genutzt. Sei in einem Petrinetz die Menge der verketteten Transaktionen T, dann ist eine ereichbare Markierung ein Zustand der durch eine dieser Transaktionen erreicht werden kann. Die Erreichbarkeitsmenge enthält die ANfangsmarkierung und alle erreichbaren Markierungen von dieser Anfangsmarkierung.  
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/erreichbarkeitsgrapf.png}
    \end{figure}\newline
    Eine \textbf{Verklemmung} existiert, wenn es keine Nachfolgemarkierung für eine Markierung gibt. \textbf{Teilweise Verklemmung} existiert, wenn eine Markierung nicht von jeder anderen Markierung aus erreichbar ist.
\subsection{Prozesskoordination}
\subsubsection{Nebenläufigkeit von Prozessen}
    \textbf{Unabhängige Abläufe} sind gegebenenfalls parallel auszuführende Benutzeraufträge, die sich gegenseitig nicht beeinflussen. Wohingegen \textbf{abhängige Abläufe} miteinander kooperieren.\newline
    \textbf{Quasi-Parallelität} bezeichnet stückweise Abarbeitung von Prozesses auf einem Prozessor, wohingegen \textbf{Paralellität} mehrere Prozesse auf mehreren Prozessoren gleichzeitig bearbeitet.\newline
    Mehrere Abläufe heißen \textbf{nebenläufig}, wenn sie inhaltlich zusammenwirken, also parallel und abhängig sind.\newline
    Beim \textbf{synchronem Ablaufverhalten} sind alle Abläufe streng getaktet, so dass jeder Prozess zu einem Zeitpunkt an eine Stelle seiner Ausführung ankommt. Wohingegen \textbf{asynchrones Ablaufverhalten} ungeregelte Abläufe beschreibt.
\subsubsection{Kritische Bereiche}
    \textbf{3.2.2.1 Erzeuger/Verbraucher-Problem}\newline
    Ein Synchronisationsproblem kann auftregen, wenn der Verbraucher nicht auf Daten zugreifen darf, die noch nicht produziert wurden, der Erzeuger keine Daten ablegen darf, falls der Speicherplatz voll ist, oder der Speicher nicht gleichzeitig von Erzeuger und Verbraucher verändert werden darf.\newline
    Ein \textbf{einfacher Lösungsansatz} für den dritten Punkt, den gleichzeitigen Speicherzugriff, isst das Vermeiden von Parallelität und von Abhängigkeit.\newline
    Beim \textbf{Lösungsansatz mit Ringpuffer}, welcher durch ein eindimensionales Array mit Modulo-Funktion realisiert wird, muss immer ein Speicherplatz frei bleiben.
\subsubsection{Wechselseitiger Ausschluss}
    Solange sich ein Prozess in einem kritischen Bereich befindet, kann kein anderer Prozess in diesen kritischen Bereich eintreten. Ein \textbf{kritischer Bereich} ist eine Phase, in der der Prozess gemeinsam genutzte Daten oder Betriebsmittel einsetzt.\newline
    Lösungen um keinen zweiten Prozess in einen kritischen Zustand zu lassen sind, \textbf{Mutal exclusion}, wobei höchstens ein Prozess im kritischen Bereich zugelassen wird, \textbf{Progress}, wenn sich kein Prozess im kritischen Bereich befindet wird ein Kandidat in diesen Bereich zugelassen, oder \textbf{Bounded Waiting}, wobei zwischen Anforderung eines Prozesses und dem Eintreten in den kritischen Bereich eine Zeitdauer liegen kann, damit es möglich ist Prioritätsregelungen auszuschließen und ein Prozess den kritischen Bereich wieder verlässt.\newline
    \textbf{3.2.3.1 Softwarelösungen für den wechselseitigen Ausschluss}\newline
    Hier gibt es zwei verschiedene Algorithmen, der Algorithmus von Decker mit seinen drei Ansätzen und der Algorithmus von Peterson.\newline
    Beim \textbf{ersten Ansatz des Algorithmus von Decker} wird eine geschützte, globale Variable verwendet, die anzeigt, welcher Prozess in den kritischen Bereich eintreten darf. Möchte ein Prozess in den kritischen Bereich, muss er nachschauen, ob die Variable seinen Prozess beschreibt, wenn dies der Fall ist, kann er eintreten. Kehrt dieser wieder aus dem kritischen Bereich zurück, muss er selbst einem anderen wartenden Prozess die Variable zuweisen. Probleme sind, dass die Prozesse den Bereich nur abwechselnd betreten können, bei Termination eines Prozesses im unkritischen Bereich wird der kritische Bereich blockiert und die Variable selbst kritisch ist. Die Bedingung Progress ist nicht erfüllt.\newline
    Im \textbf{zweiten Ansatz des Algorithmus von Decker} wird nicht mehr die Nummer des Folgeprozesses gespeichert, sondern jedem Prozess wird eine eigene boolean-Variable (flag) zugewiesen, die den Anspruch auf den kritischen Bereich beschreibt. Hier kann ein Prozess mehrfach hintereinander den kritischen Bereich betreten und blockiert keinen Prozess. Fällt ein Prozess zwischen Setzen auf ''true'' und Betreten des kririschen Bereichs aus, so funktioniert der Algorithmus nicht mehr. Dadurch ist die Bedingung Mutual Exclusion nicht erfüllt.\newline
    Der \textbf{dritte Ansatz des Algorithmus von Decker} wird eine flag-Variable erst auf true gesetzt und dann die andere getestet wird. Das Problem hierbei ist, dass zwei Prozesse auf true gesetzt werden können und dadurch keiner den kritischen Bereich betreten darf.\newline
    Die \textbf{korrekte Lösung des Decker-Algorithmus} betrachtet beide Prozesszustände und bringt eine Ausführungsreihenfolge ein.\newline
    Die andere Art des Algoirhtmuses ist der \textbf{Algorithmus von Peterson}. Dieser geht nach Schritten vor. Zuerst wird angezeigt, dass der kritische Bereich betreten werden will, dann wird anderen den Vortritt eingeräumt und abschließend wird der kritische Bereich betreten. Beim Vortritt gilt das letzte Wort und alle drei Bedingungen sind erfüllt.\newline 
    \\
    \textbf{3.2.3.2 Hardwarelösungen für den wechselseitigen Ausschluss}\newline
    Eine \textbf{Unterbrechungsvermeidung (Interrupt Disabling)} kann erreicht werden, wenn man Unterbrechungen unmöglich macht und preemptives Scheduling unterbunden wird. Prozesse im kritischen Bereich dürfen nich tunterbrochen werden. Dies ist gefährlich und nicht realisierbar, da keine Problembehandlungsroutine eingeschoben werden darf, außerdem funktioniert es in einer Multiprozessorumgebung nicht.\newline
    Beim \textbf{Test and Set} werden Unterbrechungsvermeidungen nicht auf den Prozessor, sondern auf Betriebsmittel bezogen. Die Bedingung Bounded Waiting ist nicht erfüllt, da ein Prozess beleibig oft durch das Scheduling zurückgestellt werden kann. Dies kann mit Hilfe des Austauschbefehls gelöst werden.\newline
\subsubsection{Semaphore}
    Die bisherigen vorgestellten atomaren Operationen der Lösung für den wechselseitigen Ausschluss sind für komplexe Aufgabenstellungen zu unübersichtlich. Deshalb werden höhersprachliche Konstrukte entwickelt.\newline
    \textbf{3.2.4.1 Das Prinzip der Semaphore}\newline
    Ein \textbf{Semaphor} ist eine Integervariable mit $\text{init}(S, \text{Anfangswert})$, $\text{wait}(S)$ (Dekrementierung) und $\text{signal}(S)$ (Inkrementierung). Meistens ist einem Semaphor eine Warteschlange zugeordnet. Diese assoziieren ein busy waiting.\newline
    Binäre Semaphoren erreichen einen wechselseitigen Ausschluss mit Anfangswert 1.\newline
    \\
    \textbf{3.2.4.2 Ablaufsteuerung mit Hilfe von Semaphoren}\newline
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/Ablaufsteuerung.png}
    \end{figure}\newline
    \\
    \textbf{3.2.4.3 Lösung des Erzeuger/Verbraucher Problems mit Hilfe von Semaphoren}\newline
    Hier muss ein exklusiver Zugriff auf das Lager mit MAX Speicherplätzen garantiert werden, dafür wird eine binäre Semaphore benötigt. Um die Nebenbedingung mit MAX Speicherplätzen zu garantieren, können zwei Semaphoren, eine, die die Anzahl der belegten Speicherplätze beschreibt, und die andere, die die Anzahl der freien Speicherplätze repräsentiert, eingesetzt werden. Initialisiert wird die zweite mit $\text{init}(p, MAX)$.\newline 
    \\
    \textbf{3.2.4.4 Lösung für das Leser/Schreiben Problem mit Hilfe von Semaphoren}\newline
    Ein Leser muss nur dann auf Eintrittserlaubnis in den kritischen Bereich warten, wenn ein Schreiber gerade aktiv ist. Dadurch kann sich aber ein Verschwörerkreis bilden, der ewig den Schreiber blockiert.\newline
    ein anderer Ansatz für eine Lösung ist, dass wenn ein Schreiber wartet, der Leser nicht in den kritischen Bereich eintreten darf, analog wie oben kann sich ein umgekehrter Verschwörerkreis bilden.\newline
    Dies zu lösen wird ein alternatierendes Verfahren eingesetzt. Eine Analogie ist eine gegenderte Rednerliste, bei der keine weitere Männer mehr zugelassen werden, wenn sich eine Frau meldet und umgekehrt.\newline
    \\
    \textbf{3.2.4.5 Das Philosophenproblem}\newline
    Hier wird jedes Stäbchen durch eine Semaphore repräsentiert. Bei quasi gleichzeitigem Beginn kann es aber zu einem Deadlock koommen, falls alle auf das linke Stäbchen warten. Um dies zu vermeiden wird vorausgesetzt, das maximal $N-1$ von $N$ Philosophen Zugriff bekommen. 
\subsubsection{Monitore}
    \textbf{3.2.5.1 Motivation der Monitore}\newline
    OBwohl Semaphore ein flexibles Tool sind, ist es schweirig in gewissen Situationen ein korrektes Programm zu erstellen, da die Operationen meinst so über dieses verstreut sind, dass es schwierig ist, ihre globale Wirkung zu erkennen. Aus diesem Grund werden Monitore eingeführt.\newline
    \\
    \textbf{3.2.5.2 Das Prinzip der Monitore}\newline
    Ein \textbf{Monitor} ist ein Objekt, das sich im wesentlichen aus einer Menge von Prozeduren auf (gemeinsam) genutzten Daten zusammensetzt. Der Monitor kann nur von einem Prozess genutzt werden. Es besteht aus einer oder mehreren Prozeduren, lokalen Daten und einer Warteschlange für ankommende Prozesse.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/monitor.png}
    \end{figure}\newline
    Es existieren weitere Funktionen zur Realisierung der Synchronisation in einem Monitor, nämlich $\text{cwait}(c)$, welcher die Ausführung eines Prozesses suspendiert bis die Bedingung c eingetreten ist, und $\text{csignal}(c)$, welcher die Ausführung eines suspendierten Prozesses, der auf c wartet, wieder aufnimmt.
\subsubsection{Message Passing}
    Für den Nachrichtenaustausch werden die Funktionen $\text{send}(\text{destinatio},\text{message})$ und $\text{receive}(\text{source}, \text{message})$ eingeführt.\newline
    \textbf{3.2.6.1 Blocking}\newline
    \begin{itemize}
        \item Blocking Send, Blocking Receive: Sendender Prozess wird bis zum Abschicken und Empfang der Nachricht vom Empfänger geblockt; Empfänger wird beim receive-Aufruf bis zum Empfang geblockt
        \item Nonblocking Send, Blocking Receive: Der Sender wird nich bis zum Empfang blockiert; effektiv, da mehrere Nachrichten an unterschiedliche Empfänger unmittelbar geschickt werden können
        \item Nonblocking-Receive-Ansätze: Probleme resultieren aus dem Warten auf die receive; Puffering macht Sinn
    \end{itemize}\newline
    \\
    \textbf{3.2.6.2 Adressing}\newline
    Die \textbf{direkte Adressierung} gibt beim Zielprozess genau an, von welchem Prozess eine Nachricht erwartet wird und es wird jede Nachricht akzeptiert. Bei der \textbf{indirekten Adressierung} werden Nachrichten über eine gemeinsam genutzte Datenstruktur geschickt, die aus einer Warteschlange besteht (Mailboxen) mit den Möglichkeiten 1:1 (1 Sender, 1 Empfänger), n:1, 1:n (Broadcast) oder n:m. Prozesse können statisch (Ports) oder dynamisch (connect/disconnect) zugeordnet werden.\newline
    Hier kann nur immer an eine Nachricht, die eine receive Operationen aufrufen, geliefert werden, bei leerer Mailbox werden alle Prozesse blockiert und wenn eine Nachricht verfügbar wird, wird nur ein Prozess aktiviert und ihm die Nachricht übertragen.
    
\newpage
\section{Ressourcenverwaltung}
\subsection{Speicher}
    Der Speicher ist als linearer Adressraum und hierarchisch organisiert. Dieser muss vom Betriebssystem verwaltet werden, da nicht alle Programme und Datenstrukturen iin den Hauptspeicher geladen werden können.
\subsubsection{Speicherverwaltung}
    \begin{itemize}
        \item Relocation: Hauptspeicher wird gemeinsam genutzt; Programme werden ausgelagert, nur Prozess-Image muss im Hauptspeicher liegen; $\rightarrow$ Referenzen müssen in physische Adressen umgewandelt werden
        \item Protection: Schutz eines jeden Prozesses; $\rightarrow$ fremde Prozesse dürfen ohne Erlaubnis keine Informationen lesen oder modifizieren
        \item Sharing: gemeinsamer Speicherbereich 
        \item Logical Organization: Hauptspeicher mit Adressraum aus Folge von Bits; Programme modular geschrieben; $\rightarrow$ modulare Organisation besser zu verwalten 
        \item Physical Organization: Einteilung in Haupt- und Hintergrundspeicher (permanente Speicherung); $\rightarrow$ Transport der Daten von Hintergrund- in Hauptspeicher und umgekehrt
    \end{itemize}
\subsubsection{Speicherpartitionierung}
    \textbf{4.1.2.1 Feste Partitionierung}\newline
    Hierbei unterscheidet man zwischen gleichgroßen und unterschiedlich großen Partitionen des Hauptspeichers. In der jeweiligen Partition kann ein Prozess geladen werden, dessen Größe kleinergleich der Partitionsgröße ist, bei voller Auslastung, können Prozesse ausgelagert werden.\newline
    Ist ein Programm zu groß für eine Partition, wird das Programm vom Programmierer in Teilprogramme zerlegt. Interne Fragmentierung kann vorkommen, wenn kleine Programme eine Partition belegen und so Platz verschwendet wird.\newline
    \\
    \textbf{4.1.2.2 Dynamische Partitionierung}\newline
    Bei der dynamischen Partitionierung ist die Anzahl und Größe der Partitionen variabel. Diese neue Speicheranforderung entspricht in der Regel in ihrer Größe nicht den bestehenden Lücken, ws als externe Fragmentierung bezeichnet wird. Um die Lücken am Besten zu füllen wird entweder Best-Fit (kleinste Lücke, in die der Prozess passt), First-Fit (erste passende Lücke) oder Next-Fit (ab letzter Belegung nächste passende Lücke).  
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/dynamische_partitionierung.png}
    \end{figure}\newline
    \\
    \textbf{4.1.2.3 Buddy-Systeme}\newline
    Feste Partitionierung begrenzt die Anzahl der aktiven Prozesse und nutzt den Speicherplatz ineffizient. Dagegen ist eine dynamische Partitionierung komplex zu verwalten und hinterlässt unnutzbare Freiräume. Ein Kompromiss sind die Buddy-Systeme.\newline
    Ein Speicher der Größe $2^N$ wird in $2^K$ große Blöcke eingeteilt. Die kleinste Speichergröße ist $2^L$ und die größte $2^U$. Im Anfangszustand wird der verfügbare Speicherplatz als einzelner Block der Größe $2^U$ betrachtet. Kommt dann eine Speicheranforderung, wird die Bedingung $2^{U-1}<S<\leq 2^U$ geprüft, bei $S>2^U$ kann die Anforderung icht erfüllt werden. Der Speicherblock wird in zwei Buddies der Größe $2^{U-1}$ aufgespalten und für den ersten Block wird die Bedingung $2^{U-2}<S<=2^{U-1}$ getestet. Dies wird so lange ausgeführt, bis die Speichergröße passt.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/buddysystem.png}
    \end{figure}\newline
    Die Listen müssen hierbei immer aktualisiert werden. Bei Freigabe von Buddies muss geprüft werden, ob der benachbarte Buddy auch frei ist, falls dies der Fall ist, werden beide Buddies zu einem größeren zusammengefasst. Ein Nachteil ist die interne und externe Fragmentierung.\newline
    Um der internen Fragmentierung möglichst gering zu halten, wird das \textbf{gewichtete Buddy-System} eingeführt. Dabei wird ein Block der Größe $2^{n+2}$ im Verhältnis 1:3 geteilt und der Buddy $3\cdot 2^r$ im Verhältnis 2:1. Damit erhält man flexible Buddygrößen und weniger Verschwendung von Platz, der Verwaltungsaufwand wird aber damit auch größer.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/gewichtetesbuddysystem.png}
    \end{figure}
\subsubsection{Virtueller Speicher}
    \textbf{4.1.3.1 Prinzip der Speicherverwaltung}\newline
    Der \textbf{physische Adressraum} ist die Menge der im Arbeitsspeicher physisch vorhandener Speicherplätzen. Die Speicherzellen derselben Größe werden linear angeordnet.\newline
    Der \textbf{logische Adressraum} steht dem physischen entgegen und wird benötigt, falls die Kapazität des pysischen Arbeitsspeichers nicht ausreicht.\newline
    Die Speicherverwaltung soll logische zu physischen Adressen zuordnen, abgebildet durch $L\rightarrow P$. Ist die Kardinalität von L kleiner gleich der von P, kann der logische problemlos in den physischen aufgenommen werden.\newline
    Weiterhin wird der \textbf{virtuelle Speicher} gefordert, welcher bei Rechnern, die Multiprogramming-fähig sind, mehreren Nutzern erlaubt, gleichzeitig zu arbeiten und Teile des Speichers durch Systemsoftware zu belegen.\newline
    \\
    \textbf{4.1.3.2 Datentransport zwischen Hintergrund- und Arbeitsspeicher}\newline
    Zum Datentransport werden Transporteinheiten fester Länge als \textbf{Seiten} definiert. Der logische Speicher wird in diese aufgeteilt.\newline
    Das Gegenstück im physischen Adressraum sind die \textbf{Frames}, welche dieselbe Größe haben wie die Seiten.\newline
    \\
    \textbf{4.1.3.3 Abbildung virtueller auf reale Adressen}\newline
    Eine Maschinenadresse muss die Informationen zur Nummer des Seitenrahmens und die Relativadresse (Offset) angegeben sein.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/maschinenadresse.png}
    \end{figure}\newline
    Virtuelle Adressen (Programmadressen) sind ähnlich aufgebaut.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/programmadresse.png}
    \end{figure}\newline
    Die Abbildungen der virtuellen auf die realen Adresen wird mit Hilfe einer Seitentabelle vorgenommen, welche die Seiten im Hintergrundspeicher durchnummeriert und zu jeder Seite angibt, ob und mit welcher Adresse sie im Hauptspeicher verfügbar ist.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/seitentabellen.png}
    \end{figure}
\subsubsection{Paging}
    Ein Vorteil ist die Ausschaltung der externen Fragmentierung, jedoch wird die interne Fragmentierung erhöht.\newline
    \textbf{4.1.4.1 Paging-Strategien}\newline
    \begin{itemize}
        \item Demand Paging: fehlt eine Seite im Hauptspeicher, wird diese on demand nachgeladen
        \item Demand Preparing: fehlt eine seite im Hauptspeicher, werden mehrere Seiten gleichzeitig geladen; dadurch ist die Zahl der Zugriffe auf den Hintergrundspeicher geringer
        \item Look-Ahead-Paging: bei allen möglichen Fehlern können Nachladeoperationen stattfinden
    \end{itemize}\newline
    \\
    \textbf{4.1.4.2 Seitenaustauschalgorithmen}\newline
    Folgende Vorschriften (policies) gelten für den virtuellen Speicher:
    \begin{itemize}
        \item Resident Set Management Policy: soll sich eine Anzahl an Seiten ständig im Hauptspeicher befinden
        \item Fetch Policy: bestimmt die Paging-Strategie
        \item Placement Policy: bestimmt den Speicherort der Page im Frame des Hauptspeichers
        \item Replacement Policy: sollten alle zugeordneten Frames belegt sein und eine weitere Page benötigt werden, muss entschieden werden, welche Seite ersetzt werden muss
    \end{itemize}\newline
    Im Folgenden werden verschiedene Ersetzungsstrategien der Replacement Policy betrachtet.
    \begin{itemize}
        \item OPT (Optimalstategie): wählt die am längsten nicht mehr benötigte Seite; schwierige Realisierung, da nur die nächste zu bearbeitende Seite sichtbar ist
        \item FIFO (First-In-First-Out): wählt die älteste Seite
        \item LRU (Least Recently Used): wählt die am längsten nicht mehr benutzte Seite
        \item LFU (Least Frequentliy Used): wählt die am wenigsten genutzte Seite
        \item Climb-Strategie: die unterste Seite, die beim Aufstieg der Seiten bei jedem Aufruf, an dieser Position liegt
        \item Clock-Strategie: Seiten werden als zyklische Liste betrachtet; ein Use Bit wird eingeführt, beim ersten Laden und Referenzierung bekommt dieses den Wert 1; wählt die Seite mit Use Bit 0; setzt bei der Suche nach Use Bit 0, alle Use Bit 1 auf 0 zurück
    \end{itemize}\newline
    \\
    \textbf{4.1.4.3 Minimierung von Seitenfehlern}\newline
    \textbf{Belady's Anomalie} beschreibt das Auftreten von mehr Seitenfehler bei mehr Frames im Hauptspeicher bie gleicher Größe der Frames.\newline
    \textbf{Reference String} beschreibt die Folge an Speicherzugriffen eines Prozesses.\newline
    Eine Paging-System wird durch den Reference String, den Seitenaustauschalgorithmus und die Anzahl der Seiterrahmen im Speicher.\newline
    Eine abstrakte Maschine wird damit durch eine Menge, die den Status des Speichers beschreibt, mit der Anzahl an Elementen n, die auch der Anzahl der zugeordneten virtuellen Seiten entspricht, definiert. Die Menge teilt sich in den Bereich der Seiten, die gerade im Speicher sind, und die, die schon einmal angesprochen und ausgelagert wurden. Zu Beginn ist die Menge folglich leer.\newline
    \textbf{Stack-Algorithmen} beschrieben Seitenaustauschalgorithmen, bei denen alle Seiten, die sich bei einem Speicher mit $m$ Seitenrahmen nach $r$ Speicherzugriffen im Speicher befinden, auch bei einem Speicher mit $m+1$ Seitenrahmen nach $r$ Speicherzugriffen im Speicher befinden, d.h. $M(m,r)\subset M(m+1,r)$.\newline
    Im Folgenden werden nur noch Stack-Algorithmen betrachtet.\newline
    Der \textbf{Distance String} beschreibt eine Seitenreferenz durch die Entferneung der bisherigen Position der Seite im Stack von dessen Beginn.\newline
    Um die Anzahl von Seitenfehlern vorhersagen zu können, wird folgender Algorithmus eingeführt.
    \begin{equation}
        F_m = \sum_{k=m+1}^n C_k + C_{\infty} \text{mit} C_k \text{Anzahl der Vorkommen von k bzw. unendlich im Distance String}
    \end{equation}\newline
    \\
    \textbf{4.1.4.4 Working Set Strategie}\newline
    Damit die Anzahl an Frames, die einem Prozess zugeordnet werden, variabel gestaltet werden soll, wird die Working Set Strategie eingeführt. Hierzu wird die Lifetime-Funktion betrachtet, die die mittlere Zeit zwischen aufeinanderfolgenden Seitenfehlern in Abhängigkeit von der zugeordneten Rahmenzahl m angibt.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/lifetimefunktion.png}
    \end{figure}\newline
    Dem Ansatz des Working Set liegen die Annahmen, dass die jüngste Vergangenheit eng mit der unmittelbaren Zukunft korreliert, ein Prozess benötigt die Anzahl der aktiven Seiten auch in Zukunft und über weite Bereiche verhalten sich Prozesse lokal, dass heißt, Seiten, die erst vor kurzem referenziert wurden, werden in Zukunft wahrscheinlicher referenziert, Zugrunde.\newline
    Das \textbf{Working Set} ist als $W(t,h):=\bigcup_{i=t-h+1}^t r_i$ mit t Zeit und h Zugriffen definiert.\newline
    Bei dieser Strategie kann es aber passieren, dass mehr Frames gebraucht werden als vorhanden sind, und die Frage bleibt, wie h zu wählen ist. 
\subsubsection{Segmentierungsstrategien}
    Um Lücken zu füllen, wird die Speicherverdichtung mit viel Zeitaufwand durchgeführt, deshalb werden auch Segmentierungsstrategien entworfen.\newline
    Diese können sein First Fit (FF), wobei das Segement in die erste passende Lücke gepackt wird, Best Fit (BF), in die kleinste passende Lücke, und Rotating First Fit (RFF), wie Next-Fit bei Partitionisierung. 
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/segmentierungsstrategien.png}
    \end{figure}
\subsection{E/A Verwaltung}
\subsubsection{Klassifizierung von E/A-Geräten}
    \textbf{Menschen - lesbare Geräte} sind geeignet, um mit Nutzern zu kommunizieren.\newline
    \textbf{Maschinen - lesbare Geräte} sind geeignet, um mit elektronischer Ausrüstung zu kommunizieren.\newline
    \textbf{Kommunikationsgeräte} sind geeignet, um mit entfernten Geräten zu kommunizieren.\newline
    Merkmale sind die Datenrate, die Anwendung, die Komplexität der Kontrolle, die Arten des Datentransfers und der Darstellung und die Fehlerbehandlung.
\subsubsection{E/A Techniken}
    Bei der \textbf{programmierten E/A} wird durch einen Prozess ein E/A-Befehl ausgeführt. Anders wird bei der Ausführung des E/A-Befehls im \textbf{unterbrechungsgesteuerten E/A} nicht auf diesen gewartet, sondern setzt nachfolgende Operationen fort. Der \textbf{Direct Memory Access (DMA)} kontrolliert den Datenaustausch zwischen dem Hauptspeicher und einem E/A-Modul.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/zusammenfassung_ea.png}
    \end{figure}
    Begriffe sind der \textbf{E/A-Kanal} für ein fortgeschrittenes E/A-Modul, welches bereits die Funktionalität eines Prozessors besitzt, und \textbf{E/A-Prozessor} für ein E/A-Modul mit zusätzlichem lokalem Speicher.\newline
    Eine DMA kann durch Single Bus Detached DMA, Single Bus Integrate DMA und E/A-Bus realisiert werden.\newline
    Beim \textbf{Single Bus Detached DMA} nutzen alle Module das Bussystem gemeinsam. Dies ist einfach und billig, aber nicht effizient, da für einen Transport zwei Buszyklen benötigt werden und der Neumannsche Flaschenhals eintreten kann.\newline
    Beim \textbf{Single Bus Integrate DMA} können einige der Buszyklen eingespart werden.
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/3_Kapitel/single_bus_integrate_dma.png}
    \end{figure}\newline
    Der \textbf{E/A-Bus} erweitert die Single Bus Integrate DMA, indem die E/A-Module durch einen eigenen Bus überhalb des DMA-Moduls miteinander verbunden werden. 
    
\newpage
\section{Interprozesskommunikation}
\subsection{Lokale Interprozesskommunikation}
\subsubsection{Grundlagen des Nachrichtenaustauschs}
    Es gibt verschiedene Verbindungen wie Unicast, Multicast und Broadcast. Man unterscheidet zwischen verbindungsorientierter (Kommunikation mit Vereinbarung und Nachfragen beim Empfänger, z.B. TCP) und verbindungsloser (Kommunikation ohne Vereinbarung mit dem Empfänger, z.B. IP-Protokoll) Kommunikation.\newline
    Kanäle, die zwischen zwei Prozessen aufgebaut werden und einen Strom aus Bytes übermitteln, werden \textbf{Pipes} genannt. Dazu dienen die Operationen \textit{send (Kanal, Quelldaten)} und \textit{receive (Kanal, Zieldatenbereich)}.\newline
    Ist für beide Prozesse der Zeitpunkt der Abholung der Daten unerheblich, wird von \textbf{asynchroner Kommunikation} gesprochen. Dafür werden die Daten oder eine receive-Anfrage im Kanal zwischengespeichert.\newline
    Eine \textbf{synchrone Kommunikation} erfordert hingegen Wissen aller Teilnehmer und wird sofort abgerufen. 
\subsubsection{Pipes}
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/4_Kapitel/pipeaufbau.png}
    \end{figure}\newline
    Eine Interprozesskommunikation benutzt eine Pipe. Diese können nur zwischen Prozessen eingerichtet werden, die gemeinsame Vorfahren haben. Außerdem sind sie halbduplex, dass heißt das Daten nur in eine Richtung fließen können. Die \textbf{Stream Pipes} besitzen diese zwei Einschränkungen nicht.\newline
    Eingerichtet wird eine Pipe über den Systemaufruf \textit{pipe} (\textit{fd[0] / fd[1]} sind geöffnete Filescriptoren zum Lesen / Schreiben), dann wird \textit{fork} aufgerufen, um einen Kindprozess zu generieren und dann wird beschlossen, welche Seite liest und welche schreibt.\newline
    Die Konstante \textbf{PIPE_BUF} legt die Buffergröße der Pipe fest, zuviel geschriebene Daten gehen verloren.\newline
    Eine Kommunikation zwischen zwei Kindprozessen findet wie folgt statt. Zuerst wird das Schreib-Kind kreiert, dann wird die Schreibseite der Pipe vom Elternprozess geschlossen und das Schreibkind schließt die Leseseite. Anschließend wird das Lese-Kind vom Elternprozess kreiert, die Leseseite der Pipe geschlossen und vom Lese-Kind die Schreibseite.\newline
    \begin{figure}[H]
        \centering
	    \includegraphics[width=90mm]{Skizzen/4_Kapitel/komm_zwei_kind.png}
    \end{figure}
\subsubsection{FIFOs}
    Um zwischen beliebigen Prozessen zu kommunizieren, werden \textbf{FIFOs} benötigt. Bei Prozeduraufrufen unterscheidet man zwischen Anouncements (ohne Antwort) und Invocations (mit Antwort). 
\subsubsection{Stream Pipes}
    Streampipes lösen das Problem des Halbduülexbetriebs mit der Funktion \textit{stream-pipe} und das Problem der Kommunikation zwischen unverwandten Prozessen. 
\subsubsection{Sockets}
    Sockets dienen zur Kommunikation von Prozessen in einem Netzwerk und basieren auf verschiedenen Protokollen des ISO-Modells.
\subsection{Verteilte Systeme}
\subsubsection{Einführung in Verteilte Systeme}
    Ein verteiltes System ist ein System mit räumlich verteilten Komponenten, die keinen gemeinsamen Speicher benutzen und einer dezentralen Administration unterliegen.\newline
    \textbf{5.2.1.1 Historie Verteilter Systeme}\newline
    Rechenleistung hat sich alle zwei Jahre verdoppelt, schnelle, lokale Datennetze verbinden verschiedene PCs, programmiersprachliche Verbesserungen und die Abkehr von hierarchisch aufgebauten Organisationsformen ermöglichten die Entwicklung verteilter Systeme.\newline
    \\
    \textbf{5.2.1.2 Vorteile Verteilter Systeme}\newline
    \begin{itemize}
        \item Ermöglichung der Anpassung der Größe eines Systems
        \item Existierende Systeme können von neu hinzukommenden Systemkomponenten genutzt werden
        \item Mindert Risiko der Überlastung einzelner Systemkomponenten
        \item Überschaubare Verwaltung führt zu kosteneffektiver Realisierung
        \item Eigentümer einer Ressource managt diese
        \item Autonome Bestandteile
    \end{itemize}
    Mögliche Nachteile entstehen beim Übergang zu verteilten Systemen wie ein Softwaredefizit bei komplexen Lösungen. Hinzukommende Netzwerkkomponenten können neuartige Fehler verursachen und auch der Datenschutz ist problematisch.\newline
    \\
    \textbf{5.2.1.3 Klassifikation Verteilter Systeme}\newline
    Es gibt \textbf{(non) shared memory-Systeme}, \textbf{lose/fest gekoppelte Systeme} und \textbf{bus-/switchbasierte Systeme}.\newline
    \\
    \textbf{5.2.1.4 Eigenschaften Verteilter Systeme}\newline
    \begin{itemize}
        \item Räumlich Trennung
        \item Parallelität 
        \item Lokale Zustandsbetrachtung müssen zusätzlich durchgeführt werden
        \item Unabhängigkeit der Komponenten
        \item Asynchronität
        \item Autoritätenmanagement
        \item Notwendigkeit kontextbasierter Namensverwaltung
        \item Migration bei Programmen zur Leistungsverbesserung
        \item Heterogenität
        \item Evolution
        \item Mobilität zur Leistungssteigerung
    \end{itemize}
    Folgende Eigenschaften müssen in einem verteilten System implementiert sind:
    \begin{itemize}
        \item Offenheit
        \item Integrierbarkeit
        \item Flexibilität
        \item Modularität
        \item Förderation
        \item Verwaltbarkeit
        \item Sicherstellung von Dienstqualitäten
        \item Sicherheit
        \item Verteilungstransparenz (Zugriffs-, Orts-, Migrations-, Replikations-, Abarbeitungs-, Ausfall-, Ressourcen-, Verbunds-, und Gruppentransparenz)
    \end{itemize}
\subsubsection{Kommunikation in Verteilte Systeme}
    \textbf{5.2.2.1 Das Client/Server Modell}\newline
    Da der Verwaltungsaufwand der sieben Schichten im OSI-Modell zu hoch ist, wird ein Grundmodell, das Client/Server-Modell eingeführt.\newline
    Die Idee ist ein Betriebssystem als Menge kooperierender Prozesse (Server) zu strukturieren, welche Clients Dienste bereitstellen. Es basiert auf einem einfachen, verbindungslosen Anfrage/Antwortprotokoll mit 3 Protokollschichten (Bitübertragungsschicht, Verbindungsschicht und Anfrage/Anwortschicht).\newline
    Entweder wird die \textbf{Adressierung} durch eine Konstante im Header definiert, der maschine-process-Adressierung oder durch Broadcsten des Lokalisierungspakets auf das der betroffene Server antwortet.\newline
    Es wird bei Kommunikationsprimitiven zwischen \textbf{blockierten} (synchronen, Prozess wird während Sendung der Nachricht blockiert) und \textbf{nicht-blockierten} (asynchronen) Primitven unterschieden.\newline
    Das \textbf{Puffering} kann als Mailbox eingeführt werden.\newline
    Die \textbf{Zuverlässigkeit} kann nach dem Prinzip des Abschickens und was danach kommt ist egal, der Bestätigung oder der Bestätigung des piggy-backing (bei Time-Out individuelle Bestätigung) funktionieren.\newline
    \\
    \textbf{5.2.2.2 Der Remote Procedure Call}\newline
    Beim \textbf{konventionalen lokalen Prozeduraufruf} wird das Programm eingelesen, Parameter in das Register kopiert und anschließend ausgeführt. Der Rückgabewert wird über call-by-value/reference zurückgegeben.\newline
    Der \textbf{entfernte Prozeduraufruf} werden die Parameter nicht ins Register kopiert, sondern in einer Nachricht verpackt verschickt und dann vom Server wie ein lokaler Prozeduraufruf ausgeführt. Anschließend werden die Ergebnisse in einer Nachricht verpackt und an den Aufrufer geschickt.\newline
    Probleme können beim Lesen der Rückgabeparameter als umgekehrter Stack auftauchen.\newline
    Man unterscheidet die Fehler \textbf{Client kann Server nicht lokalisieren, Anfragenachricht vom Client an Server geht verloren, Server fällt nach Erhalt einer Anfrage aus, Antwort von Server an Client geht verloren und der Client fällt aus, nachdem eine Anfrage gestellt wird}.\newline
    \textbf{Implementierungsaspekte} sind:
    \begin{itemize}
        \item verbindungslos/orientiertes  Protokoll? $\rightarrow$ verbindungslos, da Sicherung von Software übernommen wird
        \item IP-Protokolle werden benutzt
        \item Bestätigung: Stop-and-wait, blast (Entscheidung, ob neu übertragen) oder selective-repeat (neue Anforderung)
        \item Flusskontrolle 
    \end{itemize}\newline
    \\
    \textbf{5.2.2.3 Kommunikation in Verteilten Systemen}\newline
    Eine \textbf{Gruppe} ist eine Menge an Prozessen, die miteinander kooperieren mit den Eigenschaften des Multicastings an alle Gruppenmitglieder und der dynamischen Erzeugung und Auflösung.\newline
    Die Adressierung erfolgt durch die Einrichtung einer eigenen Multicastadresse für die Gruppe, durch Broadcasting und Entscheidung des Kerns oder durch Prädikatadressierung.\newline
    Zwei Eigenschaften sind für eine einfache Benutzung von Gruppenkommunikation wichtig. Die Atomarität (an alle oder an kein Mitglied) und die Nachrichtenreihenfolgeeinhaltung. 
\end{document}